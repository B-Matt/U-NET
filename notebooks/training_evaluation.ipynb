{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import string\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from typing import List\n",
    "from predict2 import Prediction\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from unet.model import UNet\n",
    "from utils.dataset import Dataset\n",
    "from utils.rgb import rgb2mask, mask2rgb, LABEL_COLORS\n",
    "from utils.plots import plot_img_and_mask\n",
    "\n",
    "# Logging\n",
    "from utils.logging import logging\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_image(bbox_list, width, height):\n",
    "    image = np.zeros((width, height, 1), np.uint8)\n",
    "    for box in bbox_list:\n",
    "        x, y, w, h = box\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (255), -1)\n",
    "    return image\n",
    "\n",
    "def retrieve_bounding_boxes(input_image):\n",
    "    count, hierarchy = cv2.findContours(input_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    bounding_boxes: List = []\n",
    "    for contours in count:\n",
    "        x, y, w, h = cv2.boundingRect(contours)\n",
    "        if (w * h > 40):\n",
    "            bounding_boxes.append([x, y, w, h])\n",
    "    return bounding_boxes, len(bounding_boxes)\n",
    "\n",
    "def preload_image_data(data_dir: string, img_dir: string, is_mask: bool = False):\n",
    "    dataset_files: List = []\n",
    "    with open(pathlib.Path(data_dir, 'test_dataset.txt'), mode='r', encoding='utf-8') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            path = pathlib.Path(data_dir, img_dir, line.strip(), f'Image/{line.strip()}.png' if is_mask == False else f'Mask/0.png')\n",
    "\n",
    "            # Load image\n",
    "            img = cv2.imread(str(path))\n",
    "            img = Dataset._resize_and_pad(img, (patch_size, patch_size), (0, 0, 0))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if is_mask:\n",
    "                img = rgb2mask(img)\n",
    "            dataset_files.append(img)\n",
    "    return dataset_files\n",
    "\n",
    "test_imgs = preload_image_data(r'data', r'imgs', False)\n",
    "test_labels = preload_image_data(r'data', r'imgs', True)\n",
    "model_name = r'checkpoints/silvery-serenity-371/checkpoint.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_IoU(cm):\n",
    "    '''\n",
    "    Adapted from:\n",
    "        https://github.com/davidtvs/PyTorch-ENet/blob/master/metric/iou.py\n",
    "        https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/metrics.py#L2716-L2844\n",
    "    '''\n",
    "    \n",
    "    sum_over_row = cm.sum(axis=0)\n",
    "    sum_over_col = cm.sum(axis=1)\n",
    "    true_positives = np.diag(cm)\n",
    "\n",
    "    # sum_over_row + sum_over_col = 2 * true_positives + false_positives + false_negatives.\n",
    "    denominator = sum_over_row + sum_over_col - true_positives\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        iou = np.true_divide(true_positives, denominator)\n",
    "        iou[iou == np.inf] = 0.0\n",
    "        iou = np.nan_to_num(iou)\n",
    "    return iou, np.nanmean(iou)\n",
    "\n",
    "def IoU(mask_true, mask_pred, n_classes=2):\n",
    "        labels = np.arange(n_classes)\n",
    "        cm = confusion_matrix(mask_true.flatten(), mask_pred.flatten(), labels=labels)        \n",
    "        return compute_IoU(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'model_name': model_name,\n",
    "    'patch_width': patch_size,\n",
    "    'patch_height': patch_size,\n",
    "    'n_channels': 3,\n",
    "    'n_classes': 3\n",
    "}\n",
    "model = Prediction(model_params)\n",
    "model.initialize()\n",
    "\n",
    "log.info('[PREDICTION]: Model loaded!')\n",
    "log.info(f'[PREDICTION]: Starting prediction on {len(test_imgs)} image(s).')\n",
    "\n",
    "predicted_labels = []\n",
    "img_process_time_list = []\n",
    "m_ious = []\n",
    "\n",
    "batch_start_time = time.time()\n",
    "pbar = tqdm.tqdm(enumerate(test_imgs), total=len(test_imgs))\n",
    "for i, img in pbar:\n",
    "    img_start_time = time.time()\n",
    "    mask_predict = model.predict_image(img)\n",
    "    img_process_time = time.time() - img_start_time\n",
    "\n",
    "    predicted_labels.append(mask_predict)\n",
    "    img_process_time_list.append(img_process_time * 1000)\n",
    "\n",
    "pbar.close()\n",
    "batch_process_time = time.time() - batch_start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_score_multiclass(actual_class, pred_class, average = \"macro\"):\n",
    "  # Creating a set of all the unique classes using the actual class list\n",
    "  unique_class = set(actual_class)\n",
    "  roc_auc_list = [0.0, 0.0, 0.0]\n",
    "\n",
    "  for per_class in unique_class:\n",
    "    # Creating a list of all the classes except the current class \n",
    "    other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "    # Marking the current class as 1 and all other classes as 0\n",
    "    new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "    new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "\n",
    "    # Using the sklearn metrics method to calculate the roc_auc_score\n",
    "    try:\n",
    "      roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
    "    except:\n",
    "      roc_auc = 0.0    \n",
    "    roc_auc_list[int(per_class)] = roc_auc\n",
    "\n",
    "  return np.mean(roc_auc_list), roc_auc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_confusion_matrix_data(mask_true, mask_pred, n_classes=2):\n",
    "    labels = np.arange(n_classes)\n",
    "    cm = confusion_matrix(mask_true.flatten(), mask_pred.flatten(), labels=labels)        \n",
    "    \n",
    "    FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "    return FP, FN, TP, TN\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "   TP = 0\n",
    "   FP = 0\n",
    "   TN = 0\n",
    "   FN = 0\n",
    "\n",
    "   y_actual = y_actual.flatten()\n",
    "   y_hat = y_hat.flatten()\n",
    "\n",
    "   for i in range(len(y_hat)): \n",
    "      if y_actual[i]==y_hat[i]==1:\n",
    "         TP += 1\n",
    "      if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "         FP += 1\n",
    "      if y_actual[i]==y_hat[i]==0:\n",
    "         TN += 1\n",
    "      if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "         FN += 1\n",
    "\n",
    "   return TP, FP, TN, FN\n",
    "\n",
    "def get_data_list(y_actual, y_hat):\n",
    "   data_list = [] \n",
    "   true_list = []\n",
    "\n",
    "   y_actual = y_actual.flatten()\n",
    "   y_hat = y_hat.flatten()\n",
    "\n",
    "   for i in range(len(y_hat)): \n",
    "      if y_actual[i] == y_hat[i]==1:\n",
    "         TP += 1\n",
    "      if y_hat[i] == 1 and y_actual[i] != y_hat[i]:\n",
    "         FP += 1\n",
    "      if y_actual[i] == y_hat[i] == 0:\n",
    "         TN += 1\n",
    "      if y_hat[i] == 0 and y_actual[i] != y_hat[i]:\n",
    "         FN += 1\n",
    "\n",
    "   return data_list, true_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/683 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision [0.99906186 0.84162963 0.        ]\n",
      "precision1 0.9903417861830249\n",
      "precision_t [0.99906186 0.84162963 0.        ]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "precision_list = []\n",
    "recall_list = []\n",
    "auc_list = []\n",
    "iou_list = []\n",
    "\n",
    "pbar = tqdm.tqdm(enumerate(test_labels), total=len(test_labels))\n",
    "for i, label in pbar:\n",
    "    true_label = label.flatten()\n",
    "    predict_label = predicted_labels[i].flatten()\n",
    "    labels = np.arange(model_params['n_classes'])\n",
    "\n",
    "    precision_t = precision_score(true_label, predict_label, average=None, labels=labels, zero_division=1)\n",
    "    # recall = recall_score(true_label, predict_label, average=None, labels=labels, zero_division=1)\n",
    "    # auc_s, auc_per_class = roc_auc_score_multiclass(true_label, predict_label)\n",
    "\n",
    "    FP, FN, TP, TN = getting_confusion_matrix_data(label, predicted_labels[i], model_params['n_classes'])\n",
    "    FP1, FN1, TP1, TN1 = perf_measure(label, predicted_labels[i])\n",
    "    # class_iou, mean_iou = IoU(label, predicted_labels[i], model_params['n_classes'])\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    precision1 = TP1 / (TP1 + FP1)\n",
    "\n",
    "    print('precision', precision)\n",
    "    print('precision1', precision1)\n",
    "    print('precision_t', precision_t)\n",
    "\n",
    "    # pbar.desc = f'Precision: {precision[1]:.4f} | Recall: {recall[1]:.4f} | AUC: {auc_per_class[1]:.4f} | Mean IoU: {mean_iou:.4f} | Processing Time: {img_process_time_list[i]:.3f}ms'\n",
    "\n",
    "    # precision_list.append(precision)\n",
    "    # recall_list.append(recall)\n",
    "    # auc_list.append(auc_per_class)\n",
    "    # iou_list.append(class_iou)\n",
    "    break\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "print(precision_list)\n",
    "\n",
    "# String log evaluation metrics\n",
    "# log.info(\n",
    "# f'Precision: {np.mean(precision_list):.5f} | \\\n",
    "# Recall: {np.mean(recall_list):.5f} | \\\n",
    "# AUC: {np.mean(auc_list):.5f} | \\\n",
    "# Processing Time: {batch_process_time * 1000}s'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Based eval Precision-Recall curve\n",
    "recall_list.sort()\n",
    "precision_list.sort()\n",
    "\n",
    "plt.plot(recall_list, precision_list, marker='.', color='blue', label=f'U-NET: {np.mean(auc_list):.2f}')\n",
    "plt.title('Image based evaluation Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.savefig('results/img_based_eval_precision_recall_curve.svg', format='svg', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "metrics_frame = pd.DataFrame()\n",
    "iou_frame = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Za Franka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "franko_dir = pathlib.Path(r'franko')\n",
    "images_dir = pathlib.Path(franko_dir, r'images')\n",
    "ground_truth_dir = pathlib.Path(franko_dir, r'ground_truths')\n",
    "predictions_dir = pathlib.Path(franko_dir, r'predictions')\n",
    "\n",
    "for i, img in tqdm.tqdm(enumerate(test_imgs), total=len(test_imgs)):\n",
    "    save_img = PIL.Image.fromarray(img)\n",
    "    save_img.save(str(pathlib.Path(images_dir, f'{i}.png')))\n",
    "\n",
    "for i, ground_truth in tqdm.tqdm(enumerate(test_labels), total=len(test_labels)):\n",
    "    mpimg.imsave(str(pathlib.Path(ground_truth_dir, f'{i}.png')), ground_truth, cmap='gray')\n",
    "\n",
    "for i, predicted_img in tqdm.tqdm(enumerate(predicted_labels), total=len(predicted_labels)):\n",
    "    mpimg.imsave(str(pathlib.Path(predictions_dir, f'{i}.png')), predicted_img, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d2adf67bc340d5f9c90ba7bd804dfefdd00ffba3c1bf3ec0a409b0da4e54357"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
